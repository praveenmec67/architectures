{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "O4s112IG8TZl",
        "outputId": "e718294c-4544-40e6-b7a4-54498f34ce24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nThis notebook is a pytorch implementation of transformer architecture from scratch. This implementation is based on the\\narchitecture presented in the paper 'Attention is all you need' by Google.\\n\\nThe Transformer architecture consists of an encoder and decoder. An Encoder consists of a stack of encoder layers.\\nThere are 6 encoder layers under Encoder.Each encoder layer consists two sublayers\\n\\nSubLayer 1 - contains Attention mechanism\\nSubLayer 2 - contains FeedForward Neural Network\\n\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "##################################################  Transformer Architecture  ####################################################\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "This notebook is a pytorch implementation of transformer architecture from scratch. This implementation is based on the\n",
        "architecture presented in the paper 'Attention is all you need' by Google.\n",
        "\n",
        "The Transformer architecture consists of an encoder and decoder.\n",
        "\n",
        "Encoder :\n",
        "\n",
        "An Encoder consists of a stack of encoder layers.\n",
        "The number of encoder layers is a design choice. In this notebook, we implement a 6 layer Encoder.Each encoder layer consists\n",
        "two sublayers.\n",
        "\n",
        "Sublayer 1 - Self Attention mechanism\n",
        "Sublayer 2 - FeedForward Neural Network\n",
        "\n",
        "Sublayer 1 Input - Word embedding + Positional embedding, Sublayer 1 Output - MultiHeadAttention (Sublayer 1 Input)\n",
        "\n",
        "Sublayer 2 Input - Normalised Residual Added Sublayer1 Output, Sublayer 2 Output - FeedForward(Relu(FeedForward(Sublayer 2 Input)))\n",
        "\n",
        "Decoder :\n",
        "\n",
        "A Decoder consists of a stack of decoder layers.\n",
        "The number of decoder layers is identical to the number of encoder layers. Each decoder layer consists of three sublayers.\n",
        "The output from the last layer of the encoder is taken as the key and value matrix input for cross attention layer of the decoder.\n",
        "\n",
        "Sublayer 1 - Self Attention mechanism\n",
        "Sublayer 2 - FeedForward Neural Network\n",
        "Sublayer 3 - Cross Attention Mechanism\n",
        "\n",
        "Sublayer 1 Input - Word embedding + Positional embedding, Sublayer 1 Output - MultiHeadAttention (Sublayer 1 Input)\n",
        "\n",
        "Sublayer 2 Input - Normalised Residual Added Sublayer1 Output, Sublayer 2 Output - FeedForward(Relu(FeedForward(Sublayer 2 Input)))\n",
        "\n",
        "Sublayer 3 Input - Normalized Residual added Sublayer 2 Output, Sublayer 3 Output - CrossAttention (Sublayer 3 Input)\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "#######################################################  End of Comments  ########################################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ],
      "metadata": {
        "id": "1kOrXZw8NzSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining all the configs needed for the architecture to be implemented\n",
        "batch_size = 1\n",
        "vocab_size = 100\n",
        "seq_len = 3\n",
        "embed_dim = 512\n",
        "ffn_dim = 2048\n",
        "n_heads = 8\n",
        "n_layers = 6\n",
        "dropout_prob = 0.1"
      ],
      "metadata": {
        "id": "q5lenOLOQGGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  A class for token embedding\n",
        "\n",
        "  Attributes\n",
        "  -----------\n",
        "    embed (tensor): Token embeddings initialized for given vocab_size and embedding dimension\n",
        "    word_embed (tensor): Token embeddings for the tokens in X\n",
        "\n",
        "  Methods\n",
        "  --------\n",
        "    forward(self,X)\n",
        "    Returns the embeddings of the token\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,embed_dim,vocab_size):\n",
        "\n",
        "    \"\"\"\n",
        "    Initializes an Embedding object\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "      embed_dim (int): Dimension of the token embedding\n",
        "      vocab_size (int): Size of the vocabulary\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.embed=nn.Embedding(vocab_size,embed_dim)\n",
        "\n",
        "  def forward(self,X):\n",
        "\n",
        "    \"\"\"\n",
        "    Returns token embeddings for the tokens\n",
        "\n",
        "    Arguements\n",
        "    -----------\n",
        "      X (tensor): input tokens\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "      word_embed (tensor): Embeddings of the tokens\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    word_embed = self.embed(X)\n",
        "\n",
        "    return word_embed"
      ],
      "metadata": {
        "id": "3-TABhfUNPd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  A class for positional embedding\n",
        "\n",
        "  Attributes\n",
        "  -----------\n",
        "    seq_len (int): Number of input tokens\n",
        "    embed_dim (int): - Dimension of the token embedding\n",
        "\n",
        "  Methods\n",
        "  --------\n",
        "    forward(self,word_embeddings)\n",
        "    Returns the sum of token and positional embeddings\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def  __init__(self,seq_len,embed_dim):\n",
        "\n",
        "    \"\"\"\n",
        "    Initializes the positional embedding object\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "      embed_dim (int): Dimension of the token embedding\n",
        "      seq_len (int): Number of input tokens\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.seq_len = seq_len\n",
        "    self.embed_dim = embed_dim\n",
        "\n",
        "    pos_embed = torch.zeros(self.seq_len,self.embed_dim)\n",
        "\n",
        "    #For each token 'pos' in a given sentence of length 'seq_len', a positional embedding vector of size\n",
        "    #'embed_dim' equal to dimension of word embedding is created. For each position 'i' in positional embedding,\n",
        "    #the function to calculate the value of the vector varies depending on whether the position is even or odd.\n",
        "    #For even position sine function is used and for odd position cos function is used.\n",
        "\n",
        "    for pos in range(seq_len):\n",
        "\n",
        "      for i in range(0,self.embed_dim,2):\n",
        "\n",
        "        pos_embed[pos,i] = math.sin(pos/(10000**((2 * i)/self.embed_dim)))\n",
        "\n",
        "        pos_embed[pos,i+1] = math.cos(pos/(10000**((2 * (i+1))/self.embed_dim)))\n",
        "\n",
        "    print('POS embedding shape before Unsqueeze : ',pos_embed.shape)\n",
        "\n",
        "    pos_embed=pos_embed.unsqueeze(0)\n",
        "\n",
        "    print('POS embedding shape after Unsqueeze : ',pos_embed.shape)\n",
        "\n",
        "\n",
        "    #register_buffer of pytorch is used to save the parameters that do not need gradient updates during back propogation.\n",
        "    #Anyhow these parameters still need to be stored and loaded into state_dict\n",
        "    #https://discuss.pytorch.org/t/what-is-the-difference-between-register-buffer-and-register-parameter-of-nn-module/32723\n",
        "\n",
        "    self.register_buffer('pos_embed',pos_embed)\n",
        "\n",
        "  def forward(self,word_embeddings):\n",
        "\n",
        "    \"\"\"\n",
        "      Returns postional embeddings\n",
        "\n",
        "      Parameters\n",
        "      -----------\n",
        "      word_embeddings (tensor): embeddings of the token\n",
        "\n",
        "      Returns\n",
        "      --------\n",
        "      sum_embeddings (tensor): Positional embeddings for individual tokens\n",
        "\n",
        "    \"\"\"\n",
        "    print(word_embeddings.shape)\n",
        "    print(self.pos_embed.shape)\n",
        "    sum_embeddings = word_embeddings+self.pos_embed\n",
        "\n",
        "    return sum_embeddings\n"
      ],
      "metadata": {
        "id": "J-uWEbvuTJMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  A class for multiheadattention mechanism\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "    seq_len (int):  Number of input tokens\n",
        "    embed_dim (int):  Dimension of the token embedding\n",
        "    n_heads (int):  Number of parallel attention heads\n",
        "    dropout_prob (float): Probability of neurons to be dropped\n",
        "\n",
        "  Methods\n",
        "  --------\n",
        "    split(self,inp_tensors)\n",
        "    Returns the split heads of attention layer\n",
        "\n",
        "    scaled_dot_product(self,Q,K,single_head_dim)\n",
        "    Returns the softmax of dot product of query and key vectors scaled by the number of heads\n",
        "\n",
        "    combine_heads(self,split_context,seq_len,n_heads,single_head_dim)\n",
        "    Returns the concatenated parallel heads of attention layer\n",
        "\n",
        "    forward(self,A,B,C)\n",
        "    Returns the context vector\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,seq_len,embed_dim,n_heads,dropout_prob):\n",
        "\n",
        "    \"\"\"\n",
        "    Initializes the multihead attention object\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "      seq_len (int): Number of input tokens\n",
        "      embed_dim (int): Dimension of the token embedding\n",
        "      n_heads (int): Number of parallel attention heads\n",
        "      dropout_prob (float): Probability of neurons to be dropped\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.seq_len = seq_len\n",
        "    self.embed_dim = embed_dim\n",
        "    self.n_heads = n_heads\n",
        "    self.single_head_dim = int(self.embed_dim/self.n_heads)\n",
        "    self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    self.W_q = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
        "    self.W_k = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
        "    self.W_v = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
        "    self.W_o = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
        "\n",
        "  def split(self,inp_tensors):\n",
        "\n",
        "    \"\"\"\n",
        "    Returns the split heads of attention layer\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "      inp_tensors (tensor): query,key and value vectors to be split into parallel heads (n_heads)\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "      split_tensors (tensor): split heads of query,key and value vectors\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    split_tensors = inp_tensors.view(self.seq_len,self.n_heads,self.single_head_dim)\n",
        "\n",
        "    return split_tensors\n",
        "\n",
        "  def scaled_dot_product(self,Q,K,single_head_dim):\n",
        "\n",
        "    \"\"\"\n",
        "    Returns the softmax of dot product of query and key vectors scaled by the number of heads\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "      Q,K (tensor): query,key vectors\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "      attention_score (tensor): attention score of seq_len,n_heads,n_heads dimension\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    #Query vector of each token's each head is multiplied with the key vectors of all the heads of the same token and other tokens of the sequence\n",
        "    #Hence the resulting attention score is a matrix of size n_heads * n_heads\n",
        "\n",
        "    product = torch.matmul(Q,K.transpose(-2,-1))\n",
        "\n",
        "    scaled_product = product/math.sqrt(single_head_dim)\n",
        "\n",
        "    attention_score = nn.functional.softmax(scaled_product,dim=-1)\n",
        "\n",
        "    return attention_score\n",
        "\n",
        "  def combine_heads(self,split_context,seq_len,n_heads,single_head_dim):\n",
        "\n",
        "    \"\"\"\n",
        "    Returns the concatenated context vector\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "      split_context (tensor): split context vector\n",
        "      seq_len (int): Number of input tokens\n",
        "      n_heads (int): Number of parallel attention heads\n",
        "      single_head_dim:  dimension of each head vector\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "      attention_score (tensor): attention score of seq_len,n_heads,n_heads dimension\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    context = split_context.view(seq_len,n_heads * single_head_dim)\n",
        "\n",
        "    return context\n",
        "\n",
        "  def forward(self,A,B,C):\n",
        "\n",
        "    \"\"\"\n",
        "      Returns context\n",
        "\n",
        "      Parameters\n",
        "      -----------\n",
        "      A (tensor): can be either encoder or decoder input - word embeddings (token embeddings + pos embeddings)\n",
        "      B,C (tensor): in case of encoder - word embeddings (token embeddings + pos embeddings)\n",
        "      B,C (tensor): in case of decoder - encoder output\n",
        "\n",
        "      Returns\n",
        "      --------\n",
        "      context (tensor): context vector\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Q = self.split(self.W_q(A))\n",
        "    K = self.split(self.W_k(B))\n",
        "    V = self.split(self.W_v(C))\n",
        "\n",
        "    attention_score = self.scaled_dot_product(Q,K,self.single_head_dim)\n",
        "\n",
        "    #context = torch.matmul(attention_score,V)\n",
        "\n",
        "    split_context = attention_score @ V\n",
        "\n",
        "    context = self.dropout(self.combine_heads(split_context,self.seq_len,self.n_heads,self.single_head_dim))\n",
        "\n",
        "    return context\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ikcFMLdl-o_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  A class for feedforward mechanism\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "    embed_dim (int):  Dimension of the token embedding\n",
        "    ffn_dim (int):  Dimension of the feedforward network\n",
        "    dropout (Dropout): Dropout layer\n",
        "    ffn (tensor): instance of Sequential layer class\n",
        "\n",
        "  Methods\n",
        "  --------\n",
        "    forward(self,norm_inp)\n",
        "    Returns the feedforward network output\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,embed_dim,ffn_dim,dropout_prob):\n",
        "\n",
        "    \"\"\"\n",
        "    Initializes the feedforward object\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "      embed_dim (int): Dimension of the token embedding\n",
        "      ffn_dim (int): Dimension of the feedforward network\n",
        "      dropout (Dropout): Dropout layer\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.embed_dim = embed_dim\n",
        "    self.ffn_dim = ffn_dim\n",
        "    self.dropout = nn.Dropout(dropout_prob)\n",
        "    self.ffn = nn.Sequential(nn.Linear(self.embed_dim,self.ffn_dim),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(self.ffn_dim,self.embed_dim))\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,norm_inp):\n",
        "\n",
        "      \"\"\"\n",
        "      Returns feedforward output vector\n",
        "\n",
        "      Parameters\n",
        "      -----------\n",
        "      norm_inp (tensor): normalised input from multihead attention sublayer\n",
        "\n",
        "      Returns\n",
        "      --------\n",
        "      ffn_out (tensor): feedforward output vector\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ffn_out = self.dropout(self.ffn(norm_inp))\n",
        "\n",
        "    return ffn_out\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CDTUchHVhU_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  A class for encoder layer\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "    attention (Attention):  instance of Class MultiHeadAttention\n",
        "    norm (tensor):  instance of Class LayerNormalisation\n",
        "    dropout (Dropout): instance of Dropout layer\n",
        "    feedforward (Feedforward): instance of Class Feedforward\n",
        "\n",
        "  Methods\n",
        "  --------\n",
        "    forward(self,X,X,X)\n",
        "    Returns the encoder layer output\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,n_layers,n_heads,seq_len,embed_dim,ffn_dim,dropout_prob):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout_prob)\n",
        "    self.attention = MultiHeadAttention(seq_len,embed_dim,n_heads,dropout_prob)\n",
        "    self.norm = nn.LayerNorm(embed_dim)\n",
        "    self.feedforward = FeedForward(embed_dim,ffn_dim,dropout_prob)\n",
        "\n",
        "  def forward(self,X):\n",
        "\n",
        "    \"\"\"\n",
        "    Returns the encoder layer output\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "    X (tensor): word embeddings of encoder input tokens (token embeddings + pos embeddings)\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "    enclayer_output (tensor): Output of encoder layer of dimension (batch_size,seq_len,embed_dim)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Z = self.dropout(self.attention(X,X,X))\n",
        "    print('Sublayer 1 Output : \\n\\n',Z)\n",
        "\n",
        "    normalized_context = self.norm(Z+X)\n",
        "    ffn_output = self.feedforward(normalized_context)\n",
        "    print('Sublayer 2 Output : \\n\\n',ffn_output)\n",
        "    enclayer_output = self.norm(self.dropout(ffn_output)+normalized_context)\n",
        "    print(enclayer_output.shape)\n",
        "    return enclayer_output\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Luhkr4mg-03h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  A class for decoder layer\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "    dropout (Dropout): instance of Dropout layer\n",
        "    attention (Attention):  instance of Class MultiHeadAttention\n",
        "    norm (tensor):  instance of Class LayerNorm\n",
        "    feedforward (Feedforward): instance of Class Feedforward\n",
        "\n",
        "  Methods\n",
        "  --------\n",
        "    forward(self,X,X,X)\n",
        "    Returns the encoder layer output\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  def __init__(self,n_layers,n_heads,seq_len,embed_dim,ffn_dim,dropout_prob):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout_prob)\n",
        "    self.attention = MultiHeadAttention(seq_len,embed_dim,n_heads,dropout_prob)\n",
        "    self.norm = nn.LayerNorm(embed_dim)\n",
        "    self.feedforward = FeedForward(embed_dim,ffn_dim,dropout_prob)\n",
        "    self.cross_attn = MultiHeadAttention(seq_len,embed_dim,n_heads,dropout_prob)\n",
        "\n",
        "  def forward(self,Y,enc_out):\n",
        "\n",
        "    \"\"\"\n",
        "    Returns the decoder layer output\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "    Y (tensor): word embeddings of decoder input tokens (token embeddings + pos embeddings)\n",
        "    enc_out (tensor): output from last layer of encoder\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "    declayer_output (tensor): Output of decoder layer of dimension (batch_size,seq_len,embed_dim)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Z = self.dropout(self.attention(Y,enc_out,enc_out))\n",
        "    normalized_context = self.norm(Z+Y)\n",
        "\n",
        "    ffn_output = self.feedforward(normalized_context)\n",
        "    normalized_ffn_output = self.norm(self.dropout(ffn_output)+normalized_context)\n",
        "\n",
        "    # Normalised Feedforward network output is used to compute the Query vector by multiplying with Query matrix\n",
        "    # Key and Value vector are obtained by multiplying the output of encoder output with the Key and Value matrix of decoder\n",
        "    # Cross attention - attention between query matrix generated from decoder input and key and value matrix generated from encoder output\n",
        "\n",
        "    Z_cross = self.cross_attn(normalized_ffn_output,enc_out,enc_out)\n",
        "\n",
        "    declayer_output = self.norm(self.dropout(Z_cross)+normalized_ffn_output)\n",
        "\n",
        "    return declayer_output\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WvYr5K1qh-IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  A Class for Encoder block\n",
        "\n",
        "  Attributes\n",
        "  -----------\n",
        "    word_embedding (Embedding): instance of Class Embedding\n",
        "    pos_embedding (Embedding): instance of Class PositionalEmbedding\n",
        "    dropout (Dropout):  instance of Dropout layer\n",
        "    layers (ModuleList): Module list containing instances of Class EncoderLayer\n",
        "\n",
        "  Methods\n",
        "  --------\n",
        "    forward(enc_tokens)\n",
        "    Returns the output of encoder block\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,n_layers,n_heads,embed_dim,seq_len,vocab_size,dropout_prob,ffn_dim):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.word_embedding = Embedding(embed_dim,vocab_size)\n",
        "    self.pos_embedding = PositionalEmbedding(seq_len,embed_dim)\n",
        "    self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    self.layers = nn.ModuleList(EncoderLayer(n_layers,n_heads,seq_len,embed_dim,ffn_dim,dropout_prob) for layer in range(n_layers))\n",
        "\n",
        "\n",
        "  def forward(self,enc_tokens):\n",
        "\n",
        "    \"\"\"\n",
        "    Returns the encoder output\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "      enc_tokens (tensor):  input tokens for the encoder block\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "      X (tensor): returns the output of the encoder block of dimension (batch_size,seq_len,n_heads)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    word_embedding = self.word_embedding(enc_tokens)\n",
        "\n",
        "    pos_embedding = self.pos_embedding(word_embedding)\n",
        "\n",
        "    X = self.dropout(word_embedding + pos_embedding)\n",
        "\n",
        "\n",
        "\n",
        "    for layer_num,layer in enumerate(self.layers):\n",
        "\n",
        "      print('Encoder Layer '+str(layer_num+1)+' Input : \\n\\n',X)\n",
        "\n",
        "      X = layer(X)\n",
        "\n",
        "      print('Encode Layer '+str(layer_num+1)+' Output : \\n\\n',X)\n",
        "\n",
        "    print(X.shape)\n",
        "    return X\n"
      ],
      "metadata": {
        "id": "2qHiG2Ms_SZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  A Class for Decoder block\n",
        "\n",
        "  Attributes\n",
        "  -----------\n",
        "    word_embedding (Embedding): instance of Class Embedding\n",
        "    pos_embedding (Embedding): instance of Class PositionalEmbedding\n",
        "    dropout (Dropout):  instance of Dropout layer\n",
        "    layers (ModuleList): Module list containing instances of Class DecoderLayer\n",
        "\n",
        "  Methods\n",
        "  --------\n",
        "    forward(enc_tokens)\n",
        "    Returns the output of deccoder block\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self,n_layers,n_heads,embed_dim,seq_len,vocab_size,dropout_prob,ffn_dim):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.word_embedding = Embedding(embed_dim,vocab_size)\n",
        "    self.pos_embedding = PositionalEmbedding(seq_len,embed_dim)\n",
        "    self.dropout = nn.Dropout(dropout_prob)\n",
        "    self.layers = nn.ModuleList(DecoderLayer(n_layers,n_heads,seq_len,embed_dim,ffn_dim,dropout_prob) for layer in range(n_layers))\n",
        "\n",
        "  def forward(self,dec_tokens,enc_out):\n",
        "\n",
        "    \"\"\"\n",
        "    Returns the decoder output\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "      dec_tokens (tensor):  input tokens for the decoder block\n",
        "      enc_out (tensor): Output of encoder block of dimension (batch_size,seq_len,embed_dim)\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "      Y (tensor): returns the output of the decoder block of dimension (batch_size,seq_len,n_heads)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    word_embedding = self.word_embedding(dec_tokens)\n",
        "\n",
        "    pos_embedding = self.pos_embedding(word_embedding)\n",
        "\n",
        "    print(word_embedding.shape)\n",
        "    print(pos_embedding.shape)\n",
        "\n",
        "    Y = self.dropout(word_embedding+pos_embedding)\n",
        "\n",
        "    for layer_num,layer in enumerate(self.layers):\n",
        "\n",
        "      print('Decoder Layer '+str(layer_num+1)+' Input : ',Y)\n",
        "\n",
        "      Y = layer(Y,enc_out)\n",
        "\n",
        "      print('Decoder Layer'+str(layer_num+1)+' Output : ',Y)\n",
        "\n",
        "    return Y\n",
        "\n"
      ],
      "metadata": {
        "id": "jcROuy9IaS22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  A Class for transformer containing encoder and decoder\n",
        "\n",
        "  Attributes\n",
        "  -----------\n",
        "    encoder (Encoder):  instance of Class Encoder\n",
        "    decoder (Decoder):  instance of Class Decoder\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,n_layers,n_heads,embed_dim,seq_len,vocab_size,dropout_prob,ffn_dim):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = Encoder(n_layers,n_heads,embed_dim,seq_len,vocab_size,dropout_prob,ffn_dim)\n",
        "    self.decoder = Decoder(n_layers,n_heads,embed_dim,seq_len,vocab_size,dropout_prob,ffn_dim)\n",
        "\n",
        "  def forward(self,enc_tokens,dec_tokens):\n",
        "\n",
        "    \"\"\"\n",
        "    Returns the output of the decoder\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "      enc_tokens (tensor):  Input tokens of encoder\n",
        "      dec_tokens (tensor):  Input tokens of decoder\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "      dec_out (tensor): Output of decoder\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    self.enc_tokens = enc_tokens\n",
        "\n",
        "    self.dec_tokens = dec_tokens\n",
        "\n",
        "    enc_out = self.encoder(self.enc_tokens)\n",
        "\n",
        "    dec_out = self.decoder(self.dec_tokens,enc_out)\n",
        "\n",
        "    return dec_out\n",
        "\n"
      ],
      "metadata": {
        "id": "a9Z3tbmf_AJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer(n_layers,n_heads,embed_dim,seq_len,vocab_size,dropout_prob,ffn_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7rGg2JR_5qL",
        "outputId": "ffb53506-ccb6-4671-ee5a-e945d84ddeba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS embedding shape before Unsqueeze :  torch.Size([3, 512])\n",
            "POS embedding shape after Unsqueeze :  torch.Size([1, 3, 512])\n",
            "POS embedding shape before Unsqueeze :  torch.Size([3, 512])\n",
            "POS embedding shape after Unsqueeze :  torch.Size([1, 3, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thopGzqwAMhE",
        "outputId": "354a81aa-173b-486e-d5cf-9773cf3a9975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (word_embedding): Embedding(\n",
              "      (embed): Embedding(100, 512)\n",
              "    )\n",
              "    (pos_embedding): PositionalEmbedding()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x EncoderLayer(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (attention): MultiHeadAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (W_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (W_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (W_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (W_o): Linear(in_features=512, out_features=512, bias=False)\n",
              "        )\n",
              "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (feedforward): FeedForward(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (ffn): Sequential(\n",
              "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (1): ReLU()\n",
              "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (word_embedding): Embedding(\n",
              "      (embed): Embedding(100, 512)\n",
              "    )\n",
              "    (pos_embedding): PositionalEmbedding()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x DecoderLayer(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (attention): MultiHeadAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (W_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (W_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (W_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (W_o): Linear(in_features=512, out_features=512, bias=False)\n",
              "        )\n",
              "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (feedforward): FeedForward(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (ffn): Sequential(\n",
              "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (1): ReLU()\n",
              "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "        )\n",
              "        (cross_attn): MultiHeadAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (W_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (W_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (W_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (W_o): Linear(in_features=512, out_features=512, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_inp = torch.randint(1,vocab_size,(batch_size,seq_len))\n",
        "dec_inp = torch.randint(1,vocab_size,(batch_size,seq_len))\n",
        "\n",
        "#print(enc_inp)\n",
        "#enc_out = model(enc_inp)\n",
        "\n",
        "dec_out = model(enc_inp,dec_inp)\n",
        "print(dec_out)\n",
        "print(dec_out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t31BjrGLdphh",
        "outputId": "28056b04-5ca8-46e6-885d-dc9a59a2a2e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 512])\n",
            "torch.Size([1, 3, 512])\n",
            "Encoder Layer 1 Input : \n",
            "\n",
            " tensor([[[ 1.3754,  0.4682,  1.6899,  ...,  1.9252,  5.5338, -3.2870],\n",
            "         [ 4.0158, -1.3317, -3.0309,  ...,  2.6946, -1.2556,  0.6965],\n",
            "         [ 0.9449, -0.0000,  2.8819,  ...,  0.0000,  0.0000,  0.4235]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Sublayer 1 Output : \n",
            "\n",
            " tensor([[ 0.9896,  0.8935,  0.0000,  ...,  0.7971, -0.0000,  0.4307],\n",
            "        [ 0.3550, -0.0000, -0.5838,  ..., -1.5766,  0.4577, -0.7899],\n",
            "        [ 0.6246, -0.7355, -0.0000,  ..., -0.3283,  0.6464, -0.1441]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Sublayer 2 Output : \n",
            "\n",
            " tensor([[[-0.0000, -0.3016,  0.2931,  ..., -0.4519,  0.0657,  0.0000],\n",
            "         [ 0.0098,  0.1366,  0.3545,  ...,  0.2329, -0.0000,  0.0265],\n",
            "         [-0.0483,  0.5696,  0.1927,  ...,  0.1937,  0.0196, -0.0542]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "torch.Size([1, 3, 512])\n",
            "Encode Layer 1 Output : \n",
            "\n",
            " tensor([[[ 0.8448,  0.1068,  0.8785,  ...,  0.5096,  2.2277, -1.3180],\n",
            "         [ 1.6998, -0.5410, -1.2579,  ...,  0.5842, -0.4657, -0.1435],\n",
            "         [ 0.4091,  0.1249,  1.2085,  ..., -0.1118,  0.1022, -0.1283]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Encoder Layer 2 Input : \n",
            "\n",
            " tensor([[[ 0.8448,  0.1068,  0.8785,  ...,  0.5096,  2.2277, -1.3180],\n",
            "         [ 1.6998, -0.5410, -1.2579,  ...,  0.5842, -0.4657, -0.1435],\n",
            "         [ 0.4091,  0.1249,  1.2085,  ..., -0.1118,  0.1022, -0.1283]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Sublayer 1 Output : \n",
            "\n",
            " tensor([[-0.0182,  0.5399,  0.3368,  ...,  0.3320,  0.3846, -0.1669],\n",
            "        [-0.1219, -0.0000, -0.0599,  ...,  0.6451, -0.2380, -0.1539],\n",
            "        [ 0.0701,  0.2447, -0.2831,  ..., -0.1166,  0.0000,  0.1401]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Sublayer 2 Output : \n",
            "\n",
            " tensor([[[-0.1325,  0.0638,  0.0000,  ...,  0.0967,  0.0000,  0.1373],\n",
            "         [-0.2652,  0.4241,  0.5880,  ..., -0.2921,  0.2174, -0.0825],\n",
            "         [-0.0960,  0.0618,  0.3414,  ...,  0.0000, -0.0106, -0.0204]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "torch.Size([1, 3, 512])\n",
            "Encode Layer 2 Output : \n",
            "\n",
            " tensor([[[ 0.6104,  0.6577,  1.1065,  ...,  0.8705,  2.3776, -1.2029],\n",
            "         [ 1.2030, -0.0599, -0.6198,  ...,  0.8420, -0.4403, -0.3813],\n",
            "         [ 0.3533,  0.4188,  1.2555,  ..., -0.2226,  0.0835,  0.0081]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Encoder Layer 3 Input : \n",
            "\n",
            " tensor([[[ 0.6104,  0.6577,  1.1065,  ...,  0.8705,  2.3776, -1.2029],\n",
            "         [ 1.2030, -0.0599, -0.6198,  ...,  0.8420, -0.4403, -0.3813],\n",
            "         [ 0.3533,  0.4188,  1.2555,  ..., -0.2226,  0.0835,  0.0081]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Sublayer 1 Output : \n",
            "\n",
            " tensor([[-0.1505, -0.2218, -0.2332,  ...,  0.0619, -0.4214,  0.2233],\n",
            "        [-0.8782, -0.3025,  0.0000,  ...,  0.2173, -0.2203, -0.0000],\n",
            "        [-0.2082, -0.0708, -0.4055,  ..., -0.0000,  0.5957,  0.0926]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Sublayer 2 Output : \n",
            "\n",
            " tensor([[[-0.2726, -0.0067, -0.0000,  ..., -0.5907, -0.1733,  0.1417],\n",
            "         [-0.0122, -0.0684, -0.3244,  ..., -0.1998,  0.0910, -0.0048],\n",
            "         [-0.1414,  0.4065, -0.6458,  ..., -0.0000,  0.3450,  0.0512]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "torch.Size([1, 3, 512])\n",
            "Encode Layer 3 Output : \n",
            "\n",
            " tensor([[[ 0.1668,  0.4296,  0.8470,  ...,  0.2689,  1.6768, -0.7388],\n",
            "         [ 0.2674, -0.4343, -0.9485,  ...,  0.7521, -0.5420, -0.3838],\n",
            "         [ 0.0324,  0.8063,  0.8410,  ..., -0.1600,  1.0498,  0.1964]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Encoder Layer 4 Input : \n",
            "\n",
            " tensor([[[ 0.1668,  0.4296,  0.8470,  ...,  0.2689,  1.6768, -0.7388],\n",
            "         [ 0.2674, -0.4343, -0.9485,  ...,  0.7521, -0.5420, -0.3838],\n",
            "         [ 0.0324,  0.8063,  0.8410,  ..., -0.1600,  1.0498,  0.1964]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Sublayer 1 Output : \n",
            "\n",
            " tensor([[-0.3223,  0.1606, -0.0000,  ..., -0.0147,  0.0000, -0.1304],\n",
            "        [-0.0279, -0.2082,  0.2251,  ..., -0.2277, -0.2438,  0.1586],\n",
            "        [ 0.1193, -0.0543, -0.0164,  ..., -0.1572, -0.0000,  0.1096]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Sublayer 2 Output : \n",
            "\n",
            " tensor([[[ 0.0453,  0.2577,  0.2178,  ..., -0.0083, -0.2626,  0.2536],\n",
            "         [-0.3458,  0.3566, -0.2804,  ..., -0.0000, -0.6098,  0.0070],\n",
            "         [-0.1148,  0.1759, -0.1570,  ..., -0.2847,  0.2534,  0.1073]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "torch.Size([1, 3, 512])\n",
            "Encode Layer 4 Output : \n",
            "\n",
            " tensor([[[-0.1650,  0.7842,  0.9873,  ...,  0.1700,  1.2592, -0.6231],\n",
            "         [-0.1635, -0.2361, -0.9963,  ...,  0.4751, -1.4085, -0.2203],\n",
            "         [ 0.1849,  0.7593,  0.6599,  ..., -0.5701,  1.3168,  0.4480]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Encoder Layer 5 Input : \n",
            "\n",
            " tensor([[[-0.1650,  0.7842,  0.9873,  ...,  0.1700,  1.2592, -0.6231],\n",
            "         [-0.1635, -0.2361, -0.9963,  ...,  0.4751, -1.4085, -0.2203],\n",
            "         [ 0.1849,  0.7593,  0.6599,  ..., -0.5701,  1.3168,  0.4480]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Sublayer 1 Output : \n",
            "\n",
            " tensor([[-0.3529,  0.2343, -0.1014,  ...,  0.1479,  0.0171, -0.0000],\n",
            "        [-0.1979, -0.1959, -0.1145,  ...,  0.5929, -0.0000,  0.0568],\n",
            "        [-0.3844,  0.3942, -0.1059,  ..., -0.2386,  0.1352, -0.1925]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Sublayer 2 Output : \n",
            "\n",
            " tensor([[[ 0.0000, -0.1472,  0.0000,  ..., -0.0155, -0.2619,  0.3835],\n",
            "         [-0.2296, -0.1470,  0.5133,  ..., -0.3353, -0.1496,  0.2313],\n",
            "         [-0.0689, -0.3033,  0.4731,  ..., -0.0000,  0.0373,  0.2971]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "torch.Size([1, 3, 512])\n",
            "Encode Layer 5 Output : \n",
            "\n",
            " tensor([[[-0.5024,  0.7752,  0.8090,  ...,  0.2617,  0.8931, -0.1900],\n",
            "         [-0.5585, -0.5346, -0.4470,  ...,  0.6256, -1.4232,  0.1120],\n",
            "         [-0.2471,  0.7555,  1.0322,  ..., -0.7377,  1.3972,  0.5668]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Encoder Layer 6 Input : \n",
            "\n",
            " tensor([[[-0.5024,  0.7752,  0.8090,  ...,  0.2617,  0.8931, -0.1900],\n",
            "         [-0.5585, -0.5346, -0.4470,  ...,  0.6256, -1.4232,  0.1120],\n",
            "         [-0.2471,  0.7555,  1.0322,  ..., -0.7377,  1.3972,  0.5668]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Sublayer 1 Output : \n",
            "\n",
            " tensor([[ 0.0872,  0.4186, -0.0000,  ..., -0.2289,  0.0302, -0.3357],\n",
            "        [ 0.1143, -0.0336,  0.2998,  ...,  0.2979,  0.0767, -0.0000],\n",
            "        [ 0.0991,  0.1582, -0.1064,  ...,  0.1501,  0.1997, -0.0000]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Sublayer 2 Output : \n",
            "\n",
            " tensor([[[ 0.0243,  0.1025,  0.1305,  ..., -0.0288, -0.1235, -0.0600],\n",
            "         [ 0.1055, -0.0597, -0.1393,  ...,  0.1502, -0.0899, -0.2032],\n",
            "         [ 0.1246, -0.1589, -0.4200,  ..., -0.1898, -0.0840, -0.0415]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "torch.Size([1, 3, 512])\n",
            "Encode Layer 6 Output : \n",
            "\n",
            " tensor([[[-0.3191,  1.2678,  0.9385,  ...,  0.0421,  0.7718, -0.5131],\n",
            "         [-0.2877, -0.5816, -0.2801,  ...,  1.0214, -1.2339, -0.1108],\n",
            "         [-0.0486,  0.6527,  0.3838,  ..., -0.8031,  1.3807,  0.4502]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "torch.Size([1, 3, 512])\n",
            "torch.Size([1, 3, 512])\n",
            "torch.Size([1, 3, 512])\n",
            "torch.Size([1, 3, 512])\n",
            "torch.Size([1, 3, 512])\n",
            "Decoder Layer 1 Input :  tensor([[[-3.0605,  3.5813,  0.0347,  ...,  2.7946,  0.3709,  3.9661],\n",
            "         [ 2.2118,  0.1354,  1.7843,  ...,  2.0018, -2.0256,  2.9081],\n",
            "         [ 2.7606, -1.5064, -0.0717,  ...,  0.7144, -1.6601,  0.0505]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "Decoder Layer1 Output :  tensor([[[-2.1593,  1.5880, -0.4676,  ...,  0.9753, -0.4428,  1.5345],\n",
            "         [ 1.1692, -0.1674,  0.5828,  ...,  0.6852, -0.5822,  1.1768],\n",
            "         [ 1.4906, -1.1752, -0.2318,  ...,  0.0376, -0.6111, -0.3941]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Decoder Layer 2 Input :  tensor([[[-2.1593,  1.5880, -0.4676,  ...,  0.9753, -0.4428,  1.5345],\n",
            "         [ 1.1692, -0.1674,  0.5828,  ...,  0.6852, -0.5822,  1.1768],\n",
            "         [ 1.4906, -1.1752, -0.2318,  ...,  0.0376, -0.6111, -0.3941]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Decoder Layer2 Output :  tensor([[[-1.5348,  1.4412, -0.4994,  ...,  1.5922, -0.4278,  1.1712],\n",
            "         [ 1.5143, -0.5419, -0.1736,  ...,  0.8310, -0.5165,  1.6666],\n",
            "         [ 1.0795, -1.7889, -0.9971,  ..., -0.1509, -0.4248, -0.3924]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Decoder Layer 3 Input :  tensor([[[-1.5348,  1.4412, -0.4994,  ...,  1.5922, -0.4278,  1.1712],\n",
            "         [ 1.5143, -0.5419, -0.1736,  ...,  0.8310, -0.5165,  1.6666],\n",
            "         [ 1.0795, -1.7889, -0.9971,  ..., -0.1509, -0.4248, -0.3924]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Decoder Layer3 Output :  tensor([[[-0.8445,  1.4232, -0.3777,  ...,  1.4096, -0.7113,  0.5625],\n",
            "         [ 1.2580, -0.7423, -0.1709,  ...,  1.1171, -0.9146,  2.0451],\n",
            "         [ 0.9714, -1.3346, -0.0158,  ..., -0.3849,  0.3452, -0.4077]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Decoder Layer 4 Input :  tensor([[[-0.8445,  1.4232, -0.3777,  ...,  1.4096, -0.7113,  0.5625],\n",
            "         [ 1.2580, -0.7423, -0.1709,  ...,  1.1171, -0.9146,  2.0451],\n",
            "         [ 0.9714, -1.3346, -0.0158,  ..., -0.3849,  0.3452, -0.4077]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Decoder Layer4 Output :  tensor([[[-0.8021,  1.8884, -0.5584,  ...,  1.9105, -1.1400,  0.5889],\n",
            "         [ 1.2351, -0.0129, -0.9143,  ...,  1.1411, -0.8410,  1.7800],\n",
            "         [ 1.1254, -0.7048, -0.6144,  ...,  0.3300,  0.6060, -1.0668]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Decoder Layer 5 Input :  tensor([[[-0.8021,  1.8884, -0.5584,  ...,  1.9105, -1.1400,  0.5889],\n",
            "         [ 1.2351, -0.0129, -0.9143,  ...,  1.1411, -0.8410,  1.7800],\n",
            "         [ 1.1254, -0.7048, -0.6144,  ...,  0.3300,  0.6060, -1.0668]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Decoder Layer5 Output :  tensor([[[-0.9585,  1.1176,  0.0069,  ...,  2.4561, -1.0389,  0.2824],\n",
            "         [ 1.3109, -0.0449, -1.0751,  ...,  0.6907, -1.2107,  2.3975],\n",
            "         [ 0.7308, -0.5694, -1.0853,  ...,  0.4530,  0.3659, -0.7464]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Decoder Layer 6 Input :  tensor([[[-0.9585,  1.1176,  0.0069,  ...,  2.4561, -1.0389,  0.2824],\n",
            "         [ 1.3109, -0.0449, -1.0751,  ...,  0.6907, -1.2107,  2.3975],\n",
            "         [ 0.7308, -0.5694, -1.0853,  ...,  0.4530,  0.3659, -0.7464]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Decoder Layer6 Output :  tensor([[[-0.5370,  1.3411,  0.7134,  ...,  1.9850, -1.0184, -0.1105],\n",
            "         [ 1.4474, -0.3798, -0.6020,  ...,  0.0357, -0.6785,  0.9457],\n",
            "         [ 1.0218, -1.1079, -0.7307,  ...,  0.7601,  0.1633, -0.9114]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "tensor([[[-0.5370,  1.3411,  0.7134,  ...,  1.9850, -1.0184, -0.1105],\n",
            "         [ 1.4474, -0.3798, -0.6020,  ...,  0.0357, -0.6785,  0.9457],\n",
            "         [ 1.0218, -1.1079, -0.7307,  ...,  0.7601,  0.1633, -0.9114]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "torch.Size([1, 3, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model.__doc__\n",
        "#help(model)"
      ],
      "metadata": {
        "id": "Lrst19XyANMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FpeSsHwJI2ru"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}