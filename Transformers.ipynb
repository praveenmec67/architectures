{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "O4s112IG8TZl",
        "outputId": "671215d7-eb03-42dc-a4e1-2e47adc2f25b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"f\\nThis notebook is a pytorch implementation of transformer architecture from scratch. This implementation is based on the\\narchitecture proposed in the paper 'Attention is all you need' by Google.\\n\\nThe Transformer architecture consists of an encoder and decoder. An encoder consists of a stack of encoder layers.\\nEach encoder layer consists of the below components\\n\\n        1. Self Attention\\n        2. Residual connection\\n        3. Layer Normalization\\n        4. Feed forward neuarl network\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "##################################################  Transformer Architecture  ####################################################\n",
        "\n",
        "\"\"\"f\n",
        "This notebook is a pytorch implementation of transformer architecture from scratch. This implementation is based on the\n",
        "architecture proposed in the paper 'Attention is all you need' by Google.\n",
        "\n",
        "The Transformer architecture consists of an encoder and decoder. An encoder consists of a stack of encoder layers.\n",
        "Each encoder layer consists of the below components\n",
        "\n",
        "        1. Self Attention\n",
        "        2. Residual connection\n",
        "        3. Layer Normalization\n",
        "        4. Feed forward neuarl network\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "#######################################################  End of Comments  ########################################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ],
      "metadata": {
        "id": "1kOrXZw8NzSY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "vocab_size = 100\n",
        "seq_len = 3\n",
        "embed_dim = 512\n",
        "ffn_dim = 2048\n",
        "n_heads = 8\n",
        "n_layers = 6\n",
        "dropout_prob = 0.1"
      ],
      "metadata": {
        "id": "q5lenOLOQGGx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "    defines the embedding layer of the transformer architecture\n",
        "\n",
        "    Args:\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self,embed_dim,seq_len,vocab_size):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.embed=nn.Embedding(vocab_size,embed_dim)\n",
        "\n",
        "  def forward(self,X):\n",
        "\n",
        "    self.word_embed = self.embed(X)\n",
        "\n",
        "    return self.word_embed"
      ],
      "metadata": {
        "id": "3-TABhfUNPd7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "    Defines the positional embedding vector for the transformer architecture\n",
        "\n",
        "    For each token 'pos' in a given sentence of length 'seq_len', a positional embedding vector of size\n",
        "    'embed_dim' equal to dimension of word embedding is created. For each position 'i' in positional embedding,\n",
        "    the function to calculate the value of the vector vector varies depending on whether the position is even or odd.\n",
        "    For even position sine function is used and for odd position cos function is used.\n",
        "\n",
        "    Args :\n",
        "    seq_len - maximum length of the input tokens\n",
        "    embed_dim - dimension of the word embedding\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def  __init__(self,seq_len,embed_dim):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.seq_len = seq_len\n",
        "    self.embed_dim = embed_dim\n",
        "\n",
        "    pos_embed = torch.zeros(self.seq_len,self.embed_dim)\n",
        "\n",
        "    for pos in range(seq_len):\n",
        "\n",
        "      for i in range(0,self.embed_dim,2):\n",
        "\n",
        "        pos_embed[pos,i] = math.sin(pos/(10000**((2 * i)/self.embed_dim)))\n",
        "\n",
        "        pos_embed[pos,i+1] = math.cos(pos/(10000**((2 * (i+1))/self.embed_dim)))\n",
        "\n",
        "    print('POS embedding shape before Unsqueeze : ',pos_embed.shape)\n",
        "\n",
        "    pos_embed=pos_embed.unsqueeze(0)\n",
        "\n",
        "    print('POS embedding shape after Unsqueeze : ',pos_embed.shape)\n",
        "\n",
        "\n",
        "    #register_buffer of pytorch is used to save the parameters that do not need gradient updates during back propogation.\n",
        "    #Anyhow these parameters still need to be stored and loaded into state_dict\n",
        "    #https://discuss.pytorch.org/t/what-is-the-difference-between-register-buffer-and-register-parameter-of-nn-module/32723\n",
        "\n",
        "\n",
        "    self.register_buffer('pos_embed',pos_embed)\n",
        "\n",
        "  def forward(self):\n",
        "\n",
        "    return self.pos_embed\n"
      ],
      "metadata": {
        "id": "J-uWEbvuTJMY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self,seq_len,embed_dim,n_heads):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.seq_len = seq_len\n",
        "    self.embed_dim = embed_dim\n",
        "    self.n_heads = n_heads\n",
        "    self.single_head_dim = int(self.embed_dim/self.n_heads)\n",
        "\n",
        "    self.W_q = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
        "    self.W_k = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
        "    self.W_v = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
        "    self.W_o = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
        "\n",
        "  def split(self,inp_tensors):\n",
        "\n",
        "    return inp_tensors.view(self.seq_len,self.n_heads,self.single_head_dim)\n",
        "\n",
        "  def scaled_dot_product(self,Q,K,single_head_dim):\n",
        "\n",
        "    product = torch.matmul(Q,K.transpose(-2,-1))\n",
        "\n",
        "    scaled_product = product/math.sqrt(single_head_dim)\n",
        "\n",
        "    attention_score = nn.functional.softmax(scaled_product,dim=-1)\n",
        "\n",
        "    return attention_score\n",
        "\n",
        "  def combine_heads(self,split_context,seq_len,n_heads,single_head_dim):\n",
        "\n",
        "    context = split_context.view(seq_len,n_heads * single_head_dim)\n",
        "\n",
        "    return context\n",
        "\n",
        "  def forward(self,X):\n",
        "\n",
        "    Q = self.split(self.W_q(X))\n",
        "    K = self.split(self.W_k(X))\n",
        "    V = self.split(self.W_v(X))\n",
        "\n",
        "    attention_score = self.scaled_dot_product(Q,K,self.single_head_dim)\n",
        "\n",
        "    #context = torch.matmul(attention_score,V)\n",
        "\n",
        "    split_context = attention_score @ V\n",
        "\n",
        "    context = self.combine_heads(split_context,self.seq_len,self.n_heads,self.single_head_dim)\n",
        "\n",
        "    return context\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ikcFMLdl-o_W"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class ResidualAddition():"
      ],
      "metadata": {
        "id": "huvz430W-uyQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class LayerNormalisation(nn.LayerNorm):\n",
        "\n",
        "#   def __init__(self,embed_dim):\n",
        "\n",
        "#     self.embed_dim = embed_dim\n",
        "\n",
        "#   def forward(self,z_res):\n",
        "\n",
        "#     norm_out = nn.LayerNorm(z_res)\n",
        "\n",
        "#     print('After normalised output shape : ',norm_out.shape)\n",
        "\n",
        "#     return norm_out"
      ],
      "metadata": {
        "id": "VFkiyH70-rq-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self,embed_dim,ffn_dim):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.embed_dim = embed_dim\n",
        "    self.ffn_dim = ffn_dim\n",
        "    self.ffn = nn.Sequential(nn.Linear(self.embed_dim,self.ffn_dim),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(self.ffn_dim,self.embed_dim))\n",
        "\n",
        "    print(self.ffn[0])\n",
        "\n",
        "  # ,nn.Linear(self.ffn_dim,self.embed_dim)\n",
        "\n",
        "  def forward(self,norm_inp):\n",
        "\n",
        "    ffn_out = self.ffn(norm_inp)\n",
        "\n",
        "    return ffn_out\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CDTUchHVhU_T"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "  def __init__(self,n_layers,n_heads,seq_len,embed_dim):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "\n",
        "    self.attention=MultiHeadAttention(seq_len,embed_dim,n_heads)\n",
        "\n",
        "\n",
        "  def forward(self,token_embedding):\n",
        "\n",
        "    context = self.attention(token_embedding)\n",
        "\n",
        "    return context\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Luhkr4mg-03h"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self,n_layers,n_heads,embed_dim,seq_len,vocab_size,dropout_prob,ffn_dim):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.dropout_prob = dropout_prob\n",
        "    self.ffn_dim = ffn_dim\n",
        "\n",
        "    self.word_embedding = Embedding(embed_dim,seq_len,vocab_size)\n",
        "    self.pos_embedding = PositionalEmbedding(seq_len,embed_dim)\n",
        "    self.dropout = nn.Dropout(self.dropout_prob)\n",
        "\n",
        "    self.layers = nn.ModuleList(EncoderLayer(n_layers,n_heads,seq_len,embed_dim) for layer in range(n_layers))\n",
        "\n",
        "    self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    self.feedforward = FeedForward(embed_dim,self.ffn_dim)\n",
        "\n",
        "\n",
        "  def forward(self,tokens):\n",
        "\n",
        "    word_embedding = self.word_embedding(tokens)\n",
        "\n",
        "    pos_embedding = self.pos_embedding()\n",
        "\n",
        "    X = self.dropout(word_embedding + pos_embedding)\n",
        "\n",
        "\n",
        "    for layer_num,layer in enumerate(self.layers):\n",
        "\n",
        "\n",
        "      if layer_num == 0:\n",
        "\n",
        "        Z = self.dropout(layer(X))\n",
        "\n",
        "        Z_res = X + Z\n",
        "\n",
        "        norm_Z_res= self.norm(Z_res)\n",
        "\n",
        "        ffn_out = self.dropout(self.feedforward(norm_Z_res))\n",
        "\n",
        "        ffn_nZres = ffn_out + norm_Z_res\n",
        "\n",
        "        norm_ffn_nZres = self.norm(ffn_nZres)\n",
        "\n",
        "        prev_layer_out = norm_ffn_nZres\n",
        "\n",
        "\n",
        "      elif layer_num != 0:\n",
        "\n",
        "        Z = self.dropout(layer(prev_layer_out))\n",
        "\n",
        "        Z_res = prev_layer_out + Z\n",
        "\n",
        "        norm_Z_res = self.norm(Z_res)\n",
        "\n",
        "        ffn_out = self.dropout(self.feedforward(norm_Z_res))\n",
        "\n",
        "        ffn_nZres = ffn_out + norm_Z_res\n",
        "\n",
        "        norm_ffn_nZres = self.norm(ffn_nZres)\n",
        "\n",
        "        prev_layer_out = norm_ffn_nZres\n",
        "\n",
        "\n",
        "    return prev_layer_out\n"
      ],
      "metadata": {
        "id": "2qHiG2Ms_SZX"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "    Defines the transformer encoder and decoder\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,n_layers,n_heads,embed_dim,seq_len,vocab_size,dropout_prob,ffn_dim):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder=Encoder(n_layers,n_heads,embed_dim,seq_len,vocab_size,dropout_prob,ffn_dim)\n",
        "\n",
        "  def forward(self,tokens):\n",
        "\n",
        "    self.tokens = tokens\n",
        "\n",
        "    out = self.encoder(self.tokens)\n",
        "\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "a9Z3tbmf_AJX"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer(n_layers,n_heads,embed_dim,seq_len,vocab_size,dropout_prob,ffn_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7rGg2JR_5qL",
        "outputId": "b4f3fac2-a770-4d97-aba7-478b573b6975"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS embedding shape before Unsqueeze :  torch.Size([3, 512])\n",
            "POS embedding shape after Unsqueeze :  torch.Size([1, 3, 512])\n",
            "Linear(in_features=512, out_features=2048, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thopGzqwAMhE",
        "outputId": "08eb5703-bb47-4b46-8b6c-417895d84cb8"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (word_embedding): Embedding(\n",
              "      (embed): Embedding(100, 512)\n",
              "    )\n",
              "    (pos_embedding): PositionalEmbedding()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x EncoderLayer(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (W_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (W_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (W_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (W_o): Linear(in_features=512, out_features=512, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    (feedforward): FeedForward(\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp = torch.randint(1,vocab_size,(batch_size,seq_len))\n",
        "output = model(inp)\n",
        "print(output.shape)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t31BjrGLdphh",
        "outputId": "75c4f663-53e5-47ff-887a-17db1d2807aa"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 512])\n",
            "tensor([[[-0.5492,  0.9476,  2.1118,  ..., -1.4608,  0.7753,  1.2287],\n",
            "         [-0.2166, -0.1502, -0.0842,  ..., -1.6829,  0.6073,  1.2509],\n",
            "         [ 1.7533, -0.7659,  0.1086,  ..., -0.7527,  1.9787,  0.2499]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model.__doc__\n",
        "#help(model)"
      ],
      "metadata": {
        "id": "Lrst19XyANMf"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FpeSsHwJI2ru"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}